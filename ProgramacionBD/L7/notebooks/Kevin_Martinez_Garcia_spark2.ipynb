{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ff8db6c-0c86-40cd-b048-067f86b38e08",
   "metadata": {},
   "source": [
    "En este notebook vamos a realizar la predicción del Iris Dataset utilizando Spark MLib. Comenzaremos importando todas las dependencias necesarias para llevar a cabo el trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "56b9b807-b438-4af8-8f55-846b43b861f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, GBTClassifier, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e5a2d6-fa68-45fe-ae1e-abbcadd60839",
   "metadata": {},
   "source": [
    "Creamos la sesión con nombre 'Iris' y cargamos el .csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ab5604c0-9589-4d4b-a8e2-3b2abe7ce060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|species_id|\n",
      "+------------+-----------+------------+-----------+-------+----------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|         1|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|         1|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|         1|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|         1|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|         1|\n",
      "|         5.4|        3.9|         1.7|        0.4| setosa|         1|\n",
      "|         4.6|        3.4|         1.4|        0.3| setosa|         1|\n",
      "|         5.0|        3.4|         1.5|        0.2| setosa|         1|\n",
      "|         4.4|        2.9|         1.4|        0.2| setosa|         1|\n",
      "|         4.9|        3.1|         1.5|        0.1| setosa|         1|\n",
      "+------------+-----------+------------+-----------+-------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Iris').getOrCreate()\n",
    "df = spark.read.csv('iris.csv', header = True).cache()\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ad8c55-b482-4e7e-a3ff-fe4bfda6356a",
   "metadata": {},
   "source": [
    "Examinamos los tipos de datos de cada columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "7b50762f-51c1-42c3-a196-d1e23c9de11d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sepal_length', 'string'),\n",
       " ('sepal_width', 'string'),\n",
       " ('petal_length', 'string'),\n",
       " ('petal_width', 'string'),\n",
       " ('species', 'string'),\n",
       " ('species_id', 'string')]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009355b9-f293-4e29-8b7e-f7a5eeda93e4",
   "metadata": {},
   "source": [
    "Vemos que las columnas referentes a las dimensiones de las flores son de tipo 'string'. Vamos a tratar de transformarlas a 'float'. Además, la columna 'species_id' se pasará a formato 'int'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f0a93ec7-b0da-42e1-a030-d2f6645c07cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+----------+-------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species_id|species|\n",
      "+------------+-----------+------------+-----------+----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2|         1| setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2|         1| setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2|         1| setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2|         1| setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2|         1| setosa|\n",
      "|         5.4|        3.9|         1.7|        0.4|         1| setosa|\n",
      "|         4.6|        3.4|         1.4|        0.3|         1| setosa|\n",
      "|         5.0|        3.4|         1.5|        0.2|         1| setosa|\n",
      "|         4.4|        2.9|         1.4|        0.2|         1| setosa|\n",
      "|         4.9|        3.1|         1.5|        0.1|         1| setosa|\n",
      "|         5.4|        3.7|         1.5|        0.2|         1| setosa|\n",
      "|         4.8|        3.4|         1.6|        0.2|         1| setosa|\n",
      "|         4.8|        3.0|         1.4|        0.1|         1| setosa|\n",
      "|         4.3|        3.0|         1.1|        0.1|         1| setosa|\n",
      "|         5.8|        4.0|         1.2|        0.2|         1| setosa|\n",
      "|         5.7|        4.4|         1.5|        0.4|         1| setosa|\n",
      "|         5.4|        3.9|         1.3|        0.4|         1| setosa|\n",
      "|         5.1|        3.5|         1.4|        0.3|         1| setosa|\n",
      "|         5.7|        3.8|         1.7|        0.3|         1| setosa|\n",
      "|         5.1|        3.8|         1.5|        0.3|         1| setosa|\n",
      "+------------+-----------+------------+-----------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.select(col('sepal_length').cast('float'),\n",
    "               col('sepal_width').cast('float'),\n",
    "               col('petal_length').cast('float'),\n",
    "               col('petal_width').cast('float'),\n",
    "               col('species_id').cast('int'),\n",
    "               col('species'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa40d88-9465-44e7-92ee-7266f045d0de",
   "metadata": {},
   "source": [
    "En este caso sabemos que el Dataset no tiene datos nulos así que no nos pararemos a buscarlos. Comprobamos que los formatos son lo correctos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ab3cc311-d215-4c56-ba84-32a83829ad1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sepal_length', 'float'),\n",
       " ('sepal_width', 'float'),\n",
       " ('petal_length', 'float'),\n",
       " ('petal_width', 'float'),\n",
       " ('species_id', 'int'),\n",
       " ('species', 'string')]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1191c4-7b30-442b-8317-d19c7c15bbed",
   "metadata": {},
   "source": [
    "Vemos que las dimensiones de las flores ya se han almacenado como 'float' y el identificador de especie como 'int'. Vamos a pasara a utilizar VectorAssembler para seleccionar los 'features' de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d8af337f-6e56-4d76-bb91-c4b9575542b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+----------+-------+--------------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species_id|species|            features|\n",
      "+------------+-----------+------------+-----------+----------+-------+--------------------+\n",
      "|         5.1|        3.5|         1.4|        0.2|         1| setosa|[5.09999990463256...|\n",
      "|         4.9|        3.0|         1.4|        0.2|         1| setosa|[4.90000009536743...|\n",
      "|         4.7|        3.2|         1.3|        0.2|         1| setosa|[4.69999980926513...|\n",
      "|         4.6|        3.1|         1.5|        0.2|         1| setosa|[4.59999990463256...|\n",
      "|         5.0|        3.6|         1.4|        0.2|         1| setosa|[5.0,3.5999999046...|\n",
      "+------------+-----------+------------+-----------+----------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "required_features = ['sepal_length',\n",
    "                     'sepal_width',\n",
    "                     'petal_length',\n",
    "                     'petal_width'\n",
    "                    ]\n",
    "assembler = VectorAssembler(inputCols = required_features, outputCol = 'features')\n",
    "df = assembler.transform(df)\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d530fa-3668-40ec-bfe4-32064cbbbe51",
   "metadata": {},
   "source": [
    "En nuestro caso las es de la columna 'species' ya vienen asignadas a los enteros 0,1, y 2 en la columna 'speces_id'. En cualquier caso, vamos a borrar esta última columna y a utilizar StringIndexer para transformar 'species' en una columna categórica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "48fe8c72-3d3c-4447-a843-eb0150b135d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+--------------------+----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|            features|species_id|\n",
      "+------------+-----------+------------+-----------+-------+--------------------+----------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|[5.09999990463256...|       0.0|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|[4.90000009536743...|       0.0|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|[4.69999980926513...|       0.0|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|[4.59999990463256...|       0.0|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|[5.0,3.5999999046...|       0.0|\n",
      "|         5.4|        3.9|         1.7|        0.4| setosa|[5.40000009536743...|       0.0|\n",
      "|         4.6|        3.4|         1.4|        0.3| setosa|[4.59999990463256...|       0.0|\n",
      "|         5.0|        3.4|         1.5|        0.2| setosa|[5.0,3.4000000953...|       0.0|\n",
      "|         4.4|        2.9|         1.4|        0.2| setosa|[4.40000009536743...|       0.0|\n",
      "|         4.9|        3.1|         1.5|        0.1| setosa|[4.90000009536743...|       0.0|\n",
      "|         5.4|        3.7|         1.5|        0.2| setosa|[5.40000009536743...|       0.0|\n",
      "|         4.8|        3.4|         1.6|        0.2| setosa|[4.80000019073486...|       0.0|\n",
      "|         4.8|        3.0|         1.4|        0.1| setosa|[4.80000019073486...|       0.0|\n",
      "|         4.3|        3.0|         1.1|        0.1| setosa|[4.30000019073486...|       0.0|\n",
      "|         5.8|        4.0|         1.2|        0.2| setosa|[5.80000019073486...|       0.0|\n",
      "|         5.7|        4.4|         1.5|        0.4| setosa|[5.69999980926513...|       0.0|\n",
      "|         5.4|        3.9|         1.3|        0.4| setosa|[5.40000009536743...|       0.0|\n",
      "|         5.1|        3.5|         1.4|        0.3| setosa|[5.09999990463256...|       0.0|\n",
      "|         5.7|        3.8|         1.7|        0.3| setosa|[5.69999980926513...|       0.0|\n",
      "|         5.1|        3.8|         1.5|        0.3| setosa|[5.09999990463256...|       0.0|\n",
      "+------------+-----------+------------+-----------+-------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.drop('species_id')\n",
    "df = StringIndexer(inputCol = 'species', outputCol = 'species_id', \n",
    "                   handleInvalid = 'keep').fit(df).transform(df)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb5d47e-13cc-4161-a432-c324bdb3abac",
   "metadata": {},
   "source": [
    "Una vez hecho esto borramos la columna 'species'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "9e5a6a60-2bb5-4818-912a-50b2886fa179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+--------------------+----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|            features|species_id|\n",
      "+------------+-----------+------------+-----------+--------------------+----------+\n",
      "|         5.1|        3.5|         1.4|        0.2|[5.09999990463256...|       0.0|\n",
      "|         4.9|        3.0|         1.4|        0.2|[4.90000009536743...|       0.0|\n",
      "|         4.7|        3.2|         1.3|        0.2|[4.69999980926513...|       0.0|\n",
      "|         4.6|        3.1|         1.5|        0.2|[4.59999990463256...|       0.0|\n",
      "|         5.0|        3.6|         1.4|        0.2|[5.0,3.5999999046...|       0.0|\n",
      "|         5.4|        3.9|         1.7|        0.4|[5.40000009536743...|       0.0|\n",
      "|         4.6|        3.4|         1.4|        0.3|[4.59999990463256...|       0.0|\n",
      "|         5.0|        3.4|         1.5|        0.2|[5.0,3.4000000953...|       0.0|\n",
      "|         4.4|        2.9|         1.4|        0.2|[4.40000009536743...|       0.0|\n",
      "|         4.9|        3.1|         1.5|        0.1|[4.90000009536743...|       0.0|\n",
      "|         5.4|        3.7|         1.5|        0.2|[5.40000009536743...|       0.0|\n",
      "|         4.8|        3.4|         1.6|        0.2|[4.80000019073486...|       0.0|\n",
      "|         4.8|        3.0|         1.4|        0.1|[4.80000019073486...|       0.0|\n",
      "|         4.3|        3.0|         1.1|        0.1|[4.30000019073486...|       0.0|\n",
      "|         5.8|        4.0|         1.2|        0.2|[5.80000019073486...|       0.0|\n",
      "|         5.7|        4.4|         1.5|        0.4|[5.69999980926513...|       0.0|\n",
      "|         5.4|        3.9|         1.3|        0.4|[5.40000009536743...|       0.0|\n",
      "|         5.1|        3.5|         1.4|        0.3|[5.09999990463256...|       0.0|\n",
      "|         5.7|        3.8|         1.7|        0.3|[5.69999980926513...|       0.0|\n",
      "|         5.1|        3.8|         1.5|        0.3|[5.09999990463256...|       0.0|\n",
      "+------------+-----------+------------+-----------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.drop('species')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca98b18b-8177-4ffc-9d98-5a9152e85b79",
   "metadata": {},
   "source": [
    "Pasamos a generar la división para entrenar el modelo. Barajamos de forma aleatoria los datos y tomamos 80% entrenamiento y 20% test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "70f6e480-016f-4745-a102-f636b0691ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_data, test_data) = df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8db2f7-4389-4775-b83a-4a2cd3403be6",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a89a58e5-72f6-4539-820a-bdc9e1a67e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(labelCol = 'species_id',\n",
    "                             featuresCol = 'features',\n",
    "                             maxDepth = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945ae11a-6c75-4562-8b86-657ea419b3f9",
   "metadata": {},
   "source": [
    "Entrenamos el modelo con fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "13513c23-a34b-46b8-b159-2ddbe9199577",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = dt.fit(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c26de9-4f35-4062-85fd-a4e93ab8a399",
   "metadata": {},
   "source": [
    "Generamos la predicción de las especies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "a0c64471-7a2a-444b-8ce3-7e2d34ed43e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+--------------------+----------+------------------+-----------------+----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|            features|species_id|     rawPrediction|      probability|prediction|\n",
      "+------------+-----------+------------+-----------+--------------------+----------+------------------+-----------------+----------+\n",
      "|         4.4|        3.0|         1.3|        0.2|[4.40000009536743...|       0.0|[39.0,0.0,0.0,0.0]|[1.0,0.0,0.0,0.0]|       0.0|\n",
      "|         4.4|        3.2|         1.3|        0.2|[4.40000009536743...|       0.0|[39.0,0.0,0.0,0.0]|[1.0,0.0,0.0,0.0]|       0.0|\n",
      "|         4.6|        3.6|         1.0|        0.2|[4.59999990463256...|       0.0|[39.0,0.0,0.0,0.0]|[1.0,0.0,0.0,0.0]|       0.0|\n",
      "|         4.7|        3.2|         1.6|        0.2|[4.69999980926513...|       0.0|[39.0,0.0,0.0,0.0]|[1.0,0.0,0.0,0.0]|       0.0|\n",
      "|         4.8|        3.0|         1.4|        0.1|[4.80000019073486...|       0.0|[39.0,0.0,0.0,0.0]|[1.0,0.0,0.0,0.0]|       0.0|\n",
      "+------------+-----------+------------+-----------+--------------------+----------+------------------+-----------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred1 = model1.transform(test_data)\n",
    "pred1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b431838-2d9d-4bbe-b76c-09832da8d587",
   "metadata": {},
   "source": [
    "Pasamos a obtener la precisión del modelo utilizando MulticlassClassificationEvaluator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f15ae57b-49b9-43f0-9790-f6ec3a3d7377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with DTC =>  0.9642857142857143\n"
     ]
    }
   ],
   "source": [
    "eval1 = MulticlassClassificationEvaluator(labelCol = 'species_id', predictionCol = 'prediction',\n",
    "                                          metricName = 'accuracy')\n",
    "accuracy1 = eval1.evaluate(pred1)\n",
    "print(\"Test accuracy with DTC => \", accuracy1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83db7ea5-fe21-4130-b7b1-94272f8177db",
   "metadata": {},
   "source": [
    "Se obitene una precisión del 96.43%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699ae39a-35da-457d-8e99-2fe9a323b701",
   "metadata": {},
   "source": [
    "## Gradient-Booster tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6636eae1-4e2c-45b3-bc1d-6897d41f83e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(labelCol = 'species_id', featuresCol = 'features',\n",
    "                    maxIter = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5195ccb2-8865-4ed2-919e-371accb045a2",
   "metadata": {},
   "source": [
    "Obtenemos el modelo y la predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "31516b38-5447-4276-bb43-7d35e567e12a",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3174.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 299.0 failed 1 times, most recent failure: Lost task 0.0 in stage 299.0 (TID 288) (c4e9c0c4c96b executor driver): java.lang.IllegalArgumentException: requirement failed: GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification.\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2(GBTClassifier.scala:176)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2$adapted(GBTClassifier.scala:173)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$2(Predictor.scala:96)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1207)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2308)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2309)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1209)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1202)\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.boost(GradientBoostedTrees.scala:333)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.run(GradientBoostedTrees.scala:61)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$1(GBTClassifier.scala:209)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:170)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:58)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat jdk.internal.reflect.GeneratedMethodAccessor136.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.IllegalArgumentException: requirement failed: GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification.\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2(GBTClassifier.scala:176)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2$adapted(GBTClassifier.scala:173)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$2(Predictor.scala:96)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1207)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2308)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [179]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model2 \u001b[38;5;241m=\u001b[39m \u001b[43mgbt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m pred2 \u001b[38;5;241m=\u001b[39m model2\u001b[38;5;241m.\u001b[39mtransform(test_data)\n\u001b[1;32m      3\u001b[0m pred2\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprobability\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:335\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset):\n\u001b[0;32m--> 335\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:332\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;124;03mFits a Java model to the input dataset.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;124;03m    fitted Java model\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o3174.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 299.0 failed 1 times, most recent failure: Lost task 0.0 in stage 299.0 (TID 288) (c4e9c0c4c96b executor driver): java.lang.IllegalArgumentException: requirement failed: GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification.\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2(GBTClassifier.scala:176)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2$adapted(GBTClassifier.scala:173)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$2(Predictor.scala:96)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1207)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2308)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2309)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1209)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1202)\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.boost(GradientBoostedTrees.scala:333)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.run(GradientBoostedTrees.scala:61)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$1(GBTClassifier.scala:209)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:170)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:58)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat jdk.internal.reflect.GeneratedMethodAccessor136.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.IllegalArgumentException: requirement failed: GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification.\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2(GBTClassifier.scala:176)\n\tat org.apache.spark.ml.classification.GBTClassifier.$anonfun$train$2$adapted(GBTClassifier.scala:173)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$2(Predictor.scala:96)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1207)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2308)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "model2 = gbt.fit(training_data)\n",
    "pred2 = model2.transform(test_data)\n",
    "pred2.select('prediction','probability','features').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95753119-f043-4206-988a-f8a0c3fa602e",
   "metadata": {},
   "source": [
    "En este caso vemos como el GradientBoost Tree Classifier no es una opción adecuada para este tipo de problema, pues únicamente da soporte a clasificación binaria mientras que nuestro problema supone clasificar en tres catergorías. Omitimos por tanto el cálculo de la precisión y pasamos a hacer uso del Random Forest Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3010a6bf-735d-48a7-819f-896d926e887e",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "1934fdad-fb6e-462e-b8a0-45f1cd8618ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(labelCol = 'species_id',\n",
    "                             featuresCol = 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "8e8615b5-0f75-4102-91c7-c1fcc84c83aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = rfc.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "01fa84f6-33fd-4d25-a93b-6143177d0851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+--------------------+----------+------------------+-----------------+----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|            features|species_id|     rawPrediction|      probability|prediction|\n",
      "+------------+-----------+------------+-----------+--------------------+----------+------------------+-----------------+----------+\n",
      "|         4.4|        3.0|         1.3|        0.2|[4.40000009536743...|       0.0|[20.0,0.0,0.0,0.0]|[1.0,0.0,0.0,0.0]|       0.0|\n",
      "|         4.4|        3.2|         1.3|        0.2|[4.40000009536743...|       0.0|[20.0,0.0,0.0,0.0]|[1.0,0.0,0.0,0.0]|       0.0|\n",
      "|         4.6|        3.6|         1.0|        0.2|[4.59999990463256...|       0.0|[20.0,0.0,0.0,0.0]|[1.0,0.0,0.0,0.0]|       0.0|\n",
      "|         4.7|        3.2|         1.6|        0.2|[4.69999980926513...|       0.0|[20.0,0.0,0.0,0.0]|[1.0,0.0,0.0,0.0]|       0.0|\n",
      "|         4.8|        3.0|         1.4|        0.1|[4.80000019073486...|       0.0|[20.0,0.0,0.0,0.0]|[1.0,0.0,0.0,0.0]|       0.0|\n",
      "+------------+-----------+------------+-----------+--------------------+----------+------------------+-----------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred3 = model3.transform(test_data)\n",
    "pred3.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388150ea-a276-4696-a109-b227596cb7d2",
   "metadata": {},
   "source": [
    "Pasamos a obtener la precisión del modelo utilizando MulticlassClassificationEvaluator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "7f625904-e125-443d-ad71-9878aa0760c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with RFC =>  0.9642857142857143\n"
     ]
    }
   ],
   "source": [
    "eval3 = MulticlassClassificationEvaluator(labelCol = 'species_id', predictionCol = 'prediction',\n",
    "                                          metricName = 'accuracy')\n",
    "accuracy3 = eval3.evaluate(pred3)\n",
    "print(\"Test accuracy with RFC => \", accuracy3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77076137-66cc-4b2e-98fa-9da8c3d8da2a",
   "metadata": {},
   "source": [
    "Se obtiene una precisión del 96.42%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc89012-149d-461f-ab40-97fa5fe18e20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
